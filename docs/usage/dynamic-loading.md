https://github.com/vllm-project/vllm/issues/5491
https://medium.com/@noel.B/preventing-model-swapping-in-ollama-a-guide-to-persistent-loading-f81f1dfb858d

TODO: warn about some space still being used
TODO: mention that it's inspired by ollama
TODO: mention that un like ollama it supports keeping multiple models loaded
